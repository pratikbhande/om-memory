{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  om-memory: Observational Memory for AI Agents\n",
    "\n",
    "**Human-like memory for AI agents. 10x cheaper than RAG for conversations. Zero vector DB needed.**\n",
    "\n",
    "---\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Traditional RAG systems are great for **knowledge retrieval**, but they fail at **conversational memory**:\n",
    "\n",
    "| Turn | Traditional RAG Context | om-memory Context |\n",
    "|------|------------------------|-------------------|\n",
    "| 1 | System + Docs + Q1 (~800 tokens) | System + Docs + Q1 (~800 tokens) |\n",
    "| 5 | System + Docs + **full history** (~3,500 tokens) | System + Docs + **memory** (~1,000 tokens) |\n",
    "| 10 | System + Docs + **full history** (~8,000 tokens) ðŸ˜± | System + Docs + **memory** (~1,200 tokens) ðŸŽ‰ |\n",
    "| 50 | System + Docs + **full history** (~40,000 tokens) ðŸ’¸ | System + Docs + **memory** (~1,500 tokens) âœ… |\n",
    "\n",
    "**om-memory** observes conversations like a human â€” extracting key facts, decisions, and preferences into structured observations, then discarding the raw chat history.\n",
    "\n",
    "### Key Features\n",
    "- ðŸ”´ðŸŸ¡ðŸŸ¢ **Priority-based observations** â€” Critical facts vs nice-to-know details\n",
    "- ðŸ§µ **Thread-scoped memory** â€” Per-conversation context\n",
    "- ðŸ‘¤ **Resource-scoped memory** â€” Shared across a user's threads\n",
    "- ðŸ”„ **Rolling window** â€” Keeps recent messages, compresses older ones\n",
    "- ðŸ“Š **Built-in observability** â€” Token tracking, cost savings metrics\n",
    "- ðŸ§ª **Demo mode** â€” Low thresholds for testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"om-memory[all]==0.2.2\" nest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”‘ API Key Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Enter your OpenAI API key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # <-- Replace with your key\n",
    "\n",
    "print(\"âœ… API key configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1ï¸âƒ£ Basic Usage: Add Messages & Get Context\n",
    "\n",
    "The simplest flow â€” add messages to a thread, then retrieve the compressed context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from om_memory import ObservationalMemory\n",
    "\n",
    "# Initialize with demo mode (low thresholds for testing)\n",
    "om = ObservationalMemory()\n",
    "om.config.demo_mode = True\n",
    "om.config.observer_token_threshold = 200  # Very low for demo\n",
    "\n",
    "async def basic_demo():\n",
    "    thread_id = \"basic_demo_01\"\n",
    "    \n",
    "    # Add some conversation messages\n",
    "    messages = [\n",
    "        (\"user\", \"Hi, I'm Sarah. I'm a senior data scientist at the ML team.\"),\n",
    "        (\"assistant\", \"Hello Sarah! Great to meet you. How can I help you today?\"),\n",
    "        (\"user\", \"I need to plan my vacation. I've used 8 PTO days this year already.\"),\n",
    "        (\"assistant\", \"You have 12 PTO days remaining (20 total - 8 used). Would you like help planning?\"),\n",
    "        (\"user\", \"Yes! I want to go to Japan for 2 weeks in November.\"),\n",
    "        (\"assistant\", \"A 2-week trip to Japan in November sounds amazing! That would use 10 of your remaining 12 days.\"),\n",
    "        (\"user\", \"Perfect. Also, what's the remote work policy? I might want to work remotely for a few days before the trip.\"),\n",
    "        (\"assistant\", \"Senior employees can work remotely up to 3 days per week with manager approval.\"),\n",
    "    ]\n",
    "    \n",
    "    for role, content in messages:\n",
    "        await om.aadd_message(thread_id, role, content)\n",
    "    \n",
    "    # Get context BEFORE observation\n",
    "    ctx_before = await om.aget_context(thread_id)\n",
    "    print(\"=== CONTEXT BEFORE OBSERVATION ===\")\n",
    "    print(ctx_before)\n",
    "    print(f\"\\n(~{len(ctx_before)//4} tokens)\")\n",
    "    \n",
    "    # Force observation â€” compress the conversation\n",
    "    print(\"\\n\\nðŸ”„ Running observation (compressing conversation)...\\n\")\n",
    "    await om.aobserve(thread_id)\n",
    "    \n",
    "    # Get context AFTER observation\n",
    "    ctx_after = await om.aget_context(thread_id)\n",
    "    print(\"=== CONTEXT AFTER OBSERVATION ===\")\n",
    "    print(ctx_after)\n",
    "    print(f\"\\n(~{len(ctx_after)//4} tokens)\")\n",
    "    print(f\"\\nâœ… Compression: {len(ctx_before)//4} â†’ {len(ctx_after)//4} tokens ({(1 - len(ctx_after)/len(ctx_before))*100:.0f}% smaller)\")\n",
    "\n",
    "asyncio.run(basic_demo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2ï¸âƒ£ Resource-Scoped Memory: Cross-Thread Sharing\n",
    "\n",
    "Unlike traditional RAG, om-memory can share observations **across multiple conversations** for the same user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def resource_scoped_demo():\n",
    "    om2 = ObservationalMemory()\n",
    "    om2.config.demo_mode = True\n",
    "    om2.config.observer_token_threshold = 200\n",
    "    \n",
    "    user_id = \"user_sarah_42\"  # Resource ID = shared across all her threads\n",
    "    \n",
    "    # ---- Thread 1: HR Chat ----\n",
    "    print(\"=== Thread 1: HR Chat ===\")\n",
    "    t1 = \"hr_chat_sarah\"\n",
    "    await om2.aadd_message(t1, \"user\", \"I'm Sarah, a senior data scientist. I need to plan 2 weeks of PTO for Japan in November.\", resource_id=user_id)\n",
    "    await om2.aadd_message(t1, \"assistant\", \"Got it, Sarah! You have 12 PTO days remaining.\", resource_id=user_id)\n",
    "    await om2.aadd_message(t1, \"user\", \"I've been at the company for 6 years, so I get extra days right?\", resource_id=user_id)\n",
    "    await om2.aadd_message(t1, \"assistant\", \"Correct! With 6 years of tenure, you get 25 total PTO days.\", resource_id=user_id)\n",
    "    await om2.aobserve(t1, resource_id=user_id)\n",
    "    \n",
    "    # ---- Thread 2: IT Support ----\n",
    "    print(\"\\n=== Thread 2: IT Support (DIFFERENT conversation) ===\")\n",
    "    t2 = \"it_support_sarah\"\n",
    "    # This thread automatically gets Sarah's observations from Thread 1!\n",
    "    ctx = await om2.aget_context(t2, resource_id=user_id)\n",
    "    print(\"Context in Thread 2 (includes memory from Thread 1):\")\n",
    "    print(ctx)\n",
    "    print(\"\\nâœ… The IT support thread already knows Sarah is a senior data scientist planning a Japan trip!\")\n",
    "\n",
    "asyncio.run(resource_scoped_demo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ Rolling Window: Message Retention\n",
    "\n",
    "After observation, om-memory keeps a configurable number of recent messages (rolling window) instead of deleting everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rolling_window_demo():\n",
    "    om3 = ObservationalMemory()\n",
    "    om3.config.demo_mode = True\n",
    "    om3.config.observer_token_threshold = 100\n",
    "    om3.config.message_retention_count = 2  # Keep only last 2 messages after observation\n",
    "    \n",
    "    thread_id = \"rolling_demo\"\n",
    "    \n",
    "    # Add 6 messages\n",
    "    for i in range(1, 7):\n",
    "        await om3.aadd_message(thread_id, \"user\", f\"Message number {i} from user\")\n",
    "        await om3.aadd_message(thread_id, \"assistant\", f\"Response number {i} from assistant\")\n",
    "    \n",
    "    print(f\"Messages before observation: {len(await om3.storage.aget_messages(thread_id))}\")\n",
    "    \n",
    "    # Observe â€” this compresses and retains only the last 2 messages\n",
    "    await om3.aobserve(thread_id)\n",
    "    \n",
    "    remaining = await om3.storage.aget_messages(thread_id)\n",
    "    print(f\"Messages after observation:  {len(remaining)}\")\n",
    "    print(f\"\\nRetained messages:\")\n",
    "    for m in remaining:\n",
    "        print(f\"  {m.role}: {m.content}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Only kept the last {om3.config.message_retention_count} messages. Everything else is compressed into observations.\")\n",
    "\n",
    "asyncio.run(rolling_window_demo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ Viewing Observations: What om-memory Extracts\n",
    "\n",
    "Let's see exactly what structured observations om-memory creates from raw conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def observations_demo():\n",
    "    om4 = ObservationalMemory()\n",
    "    om4.config.demo_mode = True\n",
    "    om4.config.observer_token_threshold = 150\n",
    "    \n",
    "    thread_id = \"obs_demo\"\n",
    "    \n",
    "    # A rich conversation with lots of extractable facts\n",
    "    convos = [\n",
    "        (\"user\", \"I'm Marcus, an L4 Staff Engineer on the Platform team. Been here 8 years.\"),\n",
    "        (\"assistant\", \"Welcome Marcus! As an L4 Staff Engineer with 8 years of tenure, you're at a senior level.\"),\n",
    "        (\"user\", \"I'm considering going for L5 Principal Engineer. What do I need?\"),\n",
    "        (\"assistant\", \"For L5 promotion, you need 10+ years experience and to set company-wide technical direction.\"),\n",
    "        (\"user\", \"I'm also doing on-call next week. The rotation schedule is alphabetical, right?\"),\n",
    "        (\"assistant\", \"Yes, it rotates alphabetically. You'll get a $500 weekly stipend plus $100 per incident.\"),\n",
    "        (\"user\", \"Last thing â€” I'm presenting at KubeCon next month. What's the travel expense policy?\"),\n",
    "        (\"assistant\", \"For conferences: $2,000 learning budget, $200/night hotel, $50/day meals, economy flights.\"),\n",
    "    ]\n",
    "    \n",
    "    for role, content in convos:\n",
    "        await om4.aadd_message(thread_id, role, content)\n",
    "    \n",
    "    await om4.aobserve(thread_id)\n",
    "    \n",
    "    observations = await om4.aget_observations(thread_id)\n",
    "    \n",
    "    print(f\"ðŸ“Š Extracted {len(observations)} observations from {len(convos)} messages:\\n\")\n",
    "    for obs in observations:\n",
    "        print(f\"  {obs.priority.value} [{obs.observation_date.strftime('%H:%M')}] {obs.content}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Key facts, decisions, and plans are preserved. Raw chat history is discarded.\")\n",
    "\n",
    "asyncio.run(observations_demo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5ï¸âƒ£ Cost Comparison: RAG vs om-memory\n",
    "\n",
    "Let's quantify the actual token savings across a multi-turn conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cost_comparison():\n",
    "    om5 = ObservationalMemory()\n",
    "    om5.config.demo_mode = True\n",
    "    om5.config.observer_token_threshold = 300\n",
    "    om5.config.message_retention_count = 3\n",
    "    \n",
    "    thread_id = \"cost_demo\"\n",
    "    \n",
    "    messages = [\n",
    "        \"Hi, I'm Alex, a backend engineer.\",\n",
    "        \"How many PTO days do I get?\",\n",
    "        \"I've used 5 days. Planning a trip to Hawaii.\",\n",
    "        \"What's the remote work policy for senior engineers?\",\n",
    "        \"Tell me about the on-call rotation compensation.\",\n",
    "        \"I'm going to re:Invent. What expenses are covered?\",\n",
    "        \"Can I get business class for the flight? It's over 6 hours.\",\n",
    "        \"What mental health benefits do we have?\",\n",
    "        \"Remind me â€” how many PTO days did I say I've used?\",\n",
    "        \"Thanks for all the info! One last thing â€” what's the 401k match?\",\n",
    "    ]\n",
    "    \n",
    "    # Track tokens for both approaches\n",
    "    rag_tokens = []\n",
    "    om_tokens = []\n",
    "    full_history = \"\"\n",
    "    \n",
    "    for i, msg in enumerate(messages):\n",
    "        # Traditional RAG: full history grows every turn\n",
    "        full_history += f\"user: {msg}\\nassistant: [response]\\n\"\n",
    "        rag_context_size = len(full_history) // 4\n",
    "        rag_tokens.append(rag_context_size)\n",
    "        \n",
    "        # om-memory: compressed context stays flat\n",
    "        await om5.aadd_message(thread_id, \"user\", msg)\n",
    "        await om5.aadd_message(thread_id, \"assistant\", f\"Response to: {msg[:50]}\")\n",
    "        \n",
    "        ctx = await om5.aget_context(thread_id)\n",
    "        om_context_size = len(ctx) // 4\n",
    "        om_tokens.append(om_context_size)\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(f\"{'Turn':<6} {'RAG Tokens':<15} {'om-memory Tokens':<18} {'Savings':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(len(messages)):\n",
    "        saving = ((rag_tokens[i] - om_tokens[i]) / max(rag_tokens[i], 1)) * 100\n",
    "        print(f\"{i+1:<6} {rag_tokens[i]:<15} {om_tokens[i]:<18} {saving:.0f}%\")\n",
    "    \n",
    "    total_rag = sum(rag_tokens)\n",
    "    total_om = sum(om_tokens)\n",
    "    print(f\"\\n{'TOTAL':<6} {total_rag:<15} {total_om:<18} {((total_rag-total_om)/total_rag)*100:.0f}%\")\n",
    "    \n",
    "    # Cost at gpt-4o-mini pricing ($0.15/1M tokens)\n",
    "    rag_cost = total_rag * 0.15 / 1_000_000\n",
    "    om_cost = total_om * 0.15 / 1_000_000\n",
    "    print(f\"\\nðŸ’° Estimated cost (gpt-4o-mini):\")\n",
    "    print(f\"   RAG:       ${rag_cost:.6f}\")\n",
    "    print(f\"   om-memory: ${om_cost:.6f}\")\n",
    "    print(f\"   Saved:     ${rag_cost - om_cost:.6f}\")\n",
    "    print(f\"\\nðŸ“ˆ At 100K conversations/month: RAG=${rag_cost*100000:.2f} vs om-memory=${om_cost*100000:.2f}\")\n",
    "\n",
    "asyncio.run(cost_comparison())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6ï¸âƒ£ Context Formats: Text, Dict, JSON\n",
    "\n",
    "om-memory can output context in multiple formats for different integration patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "async def format_demo():\n",
    "    om6 = ObservationalMemory()\n",
    "    om6.config.demo_mode = True\n",
    "    om6.config.observer_token_threshold = 100\n",
    "    \n",
    "    thread_id = \"format_demo\"\n",
    "    await om6.aadd_message(thread_id, \"user\", \"My name is Priya and I work on the ML team.\")\n",
    "    await om6.aadd_message(thread_id, \"assistant\", \"Nice to meet you Priya!\")\n",
    "    await om6.aobserve(thread_id)\n",
    "    \n",
    "    # Text format\n",
    "    print(\"=== TEXT FORMAT ===\")\n",
    "    print(await om6.aget_context(thread_id, format='text'))\n",
    "    \n",
    "    # Dict format\n",
    "    print(\"\\n=== DICT FORMAT ===\")\n",
    "    ctx_dict = await om6.aget_context(thread_id, format='dict')\n",
    "    print(json.dumps(ctx_dict, indent=2, default=str))\n",
    "    \n",
    "    # JSON format\n",
    "    print(\"\\n=== JSON FORMAT ===\")\n",
    "    print(await om6.aget_context(thread_id, format='json'))\n",
    "\n",
    "asyncio.run(format_demo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Summary\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Observational Memory** | Extracts key facts from conversations into structured observations |\n",
    "| **Priority System** | ðŸ”´ Critical, ðŸŸ¡ Important, ðŸŸ¢ Info â€” like human attention |\n",
    "| **Rolling Window** | Keeps N recent messages, compresses the rest |\n",
    "| **Resource-Scoped** | Share memory across multiple threads for the same user |\n",
    "| **Demo Mode** | Low thresholds for testing and experimentation |\n",
    "| **Token Savings** | 60-80% reduction in context window tokens vs full history |\n",
    "| **Zero Vector DB** | No embedding infrastructure needed for memory (only for RAG knowledge) |\n",
    "\n",
    "### Links\n",
    "- ðŸ“¦ [PyPI: om-memory](https://pypi.org/project/om-memory/)\n",
    "- ðŸ™ [GitHub: pratik333/om-memory](https://github.com/pratik333/om-memory)\n",
    "- ðŸ“– [Documentation](https://github.com/pratik333/om-memory#readme)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
