{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† om-memory vs Traditional RAG ‚Äî Live Benchmark\n",
        "\n",
        "**om-memory** is a Python library that gives AI agents human-like long-term memory.\n",
        "Instead of stuffing the entire conversation history into every API call (like RAG),\n",
        "it compresses old messages into dense observations ‚Äî keeping context small and costs low.\n",
        "\n",
        "In this notebook, we'll:\n",
        "1. **Build a Traditional RAG chatbot** using ChromaDB + OpenAI\n",
        "2. **Build the same chatbot with om-memory**\n",
        "3. **Run them side-by-side** for 20 turns and compare token usage\n",
        "4. **Test memory accuracy** ‚Äî can om-memory recall facts after compression?\n",
        "\n",
        "> üí° You only need an **OpenAI API key** to run this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q om-memory openai chromadb matplotlib numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Enter Your API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Enter your OpenAI API key (it won't be displayed)\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "print(\"‚úÖ API key set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Set Up the Knowledge Base with ChromaDB\n",
        "\n",
        "We'll create a small company handbook as our knowledge base and index it in ChromaDB.\n",
        "This simulates a real RAG setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "# Company handbook chunks\n",
        "KB_CHUNKS = [\n",
        "    \"PTO POLICY: Full-time employees get 20 PTO days per year, accruing at 1.67 days per month from day one. Unused PTO carries over up to 5 days. Requests need 2 weeks advance notice for trips over 3 days. Q4 peak season: max 3 consecutive days unless VP-approved. Employees with 5+ years tenure get 25 days total.\",\n",
        "    \"REMOTE WORK: Hybrid model ‚Äî engineers remote on Tuesday and Thursday. Senior staff (L5+) can work up to 3 days per week remotely with manager approval. Core hours are 10AM-4PM EST. VPN is required for all remote work. Home office stipend is $1,500 per year (no rollover). Daily standup at 10:15AM EST is mandatory.\",\n",
        "    \"TRAVEL EXPENSES: Meals $50/day domestic, $75/day international. Hotels $200/night domestic, $300/night international. Submit expense reports within 30 days via Expensify. Receipts required for expenses over $25. Economy class for domestic flights; business class for international flights over 6 hours. Mileage reimbursement at $0.67/mile.\",\n",
        "    \"BENEFITS: 401k match up to 6% of salary. Health insurance options: Aetna PPO or Kaiser HMO. Dental and vision included. Life insurance at 2x annual salary. Employee Assistance Program (EAP) available 24/7. Gym membership reimbursement up to $75/month.\",\n",
        "    \"PROFESSIONAL DEVELOPMENT: $3,000/year learning budget for courses, books, and subscriptions. Conference attendance requires manager approval 30 days in advance. Professional certifications are reimbursed 100% if passed on first attempt.\"\n",
        "]\n",
        "\n",
        "# Create ChromaDB collection\n",
        "chroma_client = chromadb.Client()\n",
        "collection = chroma_client.create_collection(name=\"handbook\")\n",
        "collection.add(\n",
        "    documents=KB_CHUNKS,\n",
        "    ids=[f\"chunk_{i}\" for i in range(len(KB_CHUNKS))]\n",
        ")\n",
        "print(f\"‚úÖ Indexed {len(KB_CHUNKS)} knowledge base chunks in ChromaDB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Define the Chat Functions\n",
        "\n",
        "We'll create two chat functions:\n",
        "- **`chat_rag()`** ‚Äî Traditional RAG: retrieves relevant chunks + sends full conversation history\n",
        "- **`chat_om()`** ‚Äî om-memory: retrieves chunks + sends compressed observations + recent messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import AsyncOpenAI\n",
        "from om_memory import ObservationalMemory, OMConfig\n",
        "\n",
        "oai = AsyncOpenAI()\n",
        "\n",
        "async def chat_rag(history, query):\n",
        "    \"\"\"Traditional RAG: ChromaDB retrieval + full conversation history.\"\"\"\n",
        "    # Retrieve relevant KB chunks\n",
        "    results = collection.query(query_texts=[query], n_results=2)\n",
        "    kb_context = \"\\n\".join(results['documents'][0])\n",
        "    \n",
        "    # Build prompt with FULL history (this grows linearly!)\n",
        "    history_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in history])\n",
        "    system_prompt = (\n",
        "        f\"You are an HR assistant. Answer concisely from this knowledge base:\\n\"\n",
        "        f\"{kb_context}\\n\\nConversation history:\\n{history_text}\"\n",
        "    )\n",
        "    \n",
        "    resp = await oai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": query}],\n",
        "        max_tokens=100,\n",
        "    )\n",
        "    answer = resp.choices[0].message.content\n",
        "    history.extend([{\"role\": \"user\", \"content\": query}, {\"role\": \"assistant\", \"content\": answer}])\n",
        "    return answer, resp.usage.prompt_tokens, resp.usage.completion_tokens\n",
        "\n",
        "\n",
        "async def chat_om(om, thread_id, query):\n",
        "    \"\"\"om-memory: ChromaDB retrieval + compressed observations + recent messages.\"\"\"\n",
        "    # Retrieve relevant KB chunks\n",
        "    results = collection.query(query_texts=[query], n_results=2)\n",
        "    kb_context = \"\\n\".join(results['documents'][0])\n",
        "    \n",
        "    # Get compressed context from om-memory (observations + last few messages)\n",
        "    memory_ctx = await om.aget_context(thread_id)\n",
        "    \n",
        "    system_prompt = (\n",
        "        f\"You are an HR assistant. Answer concisely from this knowledge base:\\n\"\n",
        "        f\"{kb_context}\\n\\n{memory_ctx}\"\n",
        "    )\n",
        "    \n",
        "    resp = await oai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": query}],\n",
        "        max_tokens=100,\n",
        "    )\n",
        "    answer = resp.choices[0].message.content\n",
        "    await om.aadd_message(thread_id, \"user\", query)\n",
        "    await om.aadd_message(thread_id, \"assistant\", answer)\n",
        "    return answer, resp.usage.prompt_tokens, resp.usage.completion_tokens\n",
        "\n",
        "print(\"‚úÖ Chat functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Run the 20-Turn Comparison\n",
        "\n",
        "Now we run both approaches through the same 20 questions and track token usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "QUERIES = [\n",
        "    \"Hi, I'm Alex Chen. I'm an L5 senior engineer, been here 6 years.\",\n",
        "    \"How many PTO days do I get with my tenure?\",\n",
        "    \"Can I work remotely 3 days a week?\",\n",
        "    \"What's my home office stipend?\",\n",
        "    \"I want to attend KubeCon in Paris. What's the process?\",\n",
        "    \"The flight to Paris is 8 hours. Can I fly business class?\",\n",
        "    \"What's the international hotel allowance?\",\n",
        "    \"What about meal expenses in Paris?\",\n",
        "    \"Do I need receipts for a 20 euro coffee?\",\n",
        "    \"When do I submit expense reports?\",\n",
        "    \"Can I use my personal credit card?\",\n",
        "    \"I want a week of vacation after KubeCon. How do I request it?\",\n",
        "    \"How many carry-over days from last year?\",\n",
        "    \"What's the mileage rate to the airport?\",\n",
        "    \"Is VPN required from the Paris hotel?\",\n",
        "    \"What are the core remote work hours?\",\n",
        "    \"My manager wants to know Q4 travel restrictions.\",\n",
        "    \"What certifications does the company reimburse?\",\n",
        "    \"What's my gym reimbursement amount?\",\n",
        "    \"Give me a summary of my complete travel plan and benefits.\",\n",
        "]\n",
        "\n",
        "# --- Run Traditional RAG ---\n",
        "print(\"üî¥ Running Traditional RAG...\")\n",
        "rag_history = []\n",
        "rag_tokens = []\n",
        "rag_total = 0\n",
        "\n",
        "for i, q in enumerate(QUERIES):\n",
        "    answer, pt, ct = await chat_rag(rag_history, q)\n",
        "    rag_total += pt + ct\n",
        "    rag_tokens.append(pt)\n",
        "    print(f\"  Turn {i+1:2d}: prompt={pt:5d} tokens\")\n",
        "\n",
        "# --- Run om-memory ---\n",
        "print(\"\\nüü¢ Running om-memory...\")\n",
        "config = OMConfig(\n",
        "    observer_token_threshold=300,\n",
        "    reflector_token_threshold=1500,\n",
        "    message_retention_count=2,\n",
        "    message_token_budget=200,\n",
        "    auto_observe=True,\n",
        "    auto_reflect=True,\n",
        "    blocking_mode=True,\n",
        ")\n",
        "om = ObservationalMemory(api_key=os.environ[\"OPENAI_API_KEY\"], config=config)\n",
        "await om.ainitialize()\n",
        "thread_id = f\"colab_{int(time.time())}\"\n",
        "\n",
        "om_tokens = []\n",
        "om_total = 0\n",
        "\n",
        "for i, q in enumerate(QUERIES):\n",
        "    answer, pt, ct = await chat_om(om, thread_id, q)\n",
        "    om_total += pt + ct\n",
        "    om_tokens.append(pt)\n",
        "    obs_count = len(await om.aget_observations(thread_id))\n",
        "    print(f\"  Turn {i+1:2d}: prompt={pt:5d} tokens  |  observations={obs_count}\")\n",
        "\n",
        "stats = await om.aget_stats(thread_id)\n",
        "bg_tokens = stats.total_input_tokens + stats.total_output_tokens\n",
        "om_total_with_bg = om_total + bg_tokens\n",
        "\n",
        "print(f\"\\nüìä Results:\")\n",
        "print(f\"  RAG total:    {rag_total:,} tokens\")\n",
        "print(f\"  OM total:     {om_total_with_bg:,} tokens (incl. {bg_tokens:,} background)\")\n",
        "savings = ((rag_total - om_total_with_bg) / rag_total) * 100\n",
        "print(f\"  Savings:      {savings:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Visualize the Results\n",
        "\n",
        "Let's plot the token usage per turn ‚Äî you'll see RAG growing linearly while om-memory stays flat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Dark theme\n",
        "plt.rcParams.update({\n",
        "    'figure.facecolor': '#0d1117', 'axes.facecolor': '#161b22',\n",
        "    'axes.edgecolor': '#30363d', 'axes.labelcolor': '#e6edf3',\n",
        "    'text.color': '#e6edf3', 'xtick.color': '#8b949e',\n",
        "    'ytick.color': '#8b949e', 'grid.color': '#21262d',\n",
        "})\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "turns = range(1, len(rag_tokens) + 1)\n",
        "\n",
        "# Chart 1: Per-turn tokens\n",
        "ax1.plot(turns, rag_tokens, color='#f85149', linewidth=2.5, label='Traditional RAG', marker='o', markersize=4)\n",
        "ax1.fill_between(turns, rag_tokens, alpha=0.15, color='#f85149')\n",
        "ax1.plot(turns, om_tokens, color='#3fb950', linewidth=2.5, label='om-memory', marker='s', markersize=4)\n",
        "ax1.fill_between(turns, om_tokens, alpha=0.15, color='#3fb950')\n",
        "ax1.set_xlabel('Turn', fontsize=12)\n",
        "ax1.set_ylabel('Prompt Tokens', fontsize=12)\n",
        "ax1.set_title('Per-Turn Prompt Tokens', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Chart 2: Cumulative\n",
        "rag_cumul = np.cumsum(rag_tokens)\n",
        "om_cumul = np.cumsum(om_tokens)\n",
        "ax2.plot(turns, rag_cumul, color='#f85149', linewidth=2.5, label='RAG (cumulative)')\n",
        "ax2.plot(turns, om_cumul, color='#3fb950', linewidth=2.5, label='OM (cumulative)')\n",
        "ax2.fill_between(turns, om_cumul, rag_cumul, alpha=0.15, color='#3fb950', label='Savings')\n",
        "ax2.set_xlabel('Turn', fontsize=12)\n",
        "ax2.set_ylabel('Cumulative Prompt Tokens', fontsize=12)\n",
        "ax2.set_title('Cumulative Token Usage', fontsize=14, fontweight='bold')\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä By turn 20: RAG uses {rag_tokens[-1]:,} prompt tokens vs OM uses {om_tokens[-1]:,}\")\n",
        "print(f\"   That's a {((rag_tokens[-1] - om_tokens[-1]) / rag_tokens[-1] * 100):.0f}% reduction per turn!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Test Memory Accuracy\n",
        "\n",
        "The most critical question: **Does om-memory actually remember things?**\n",
        "\n",
        "After compression, we'll ask the agent to recall facts from early in the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recall_tests = [\n",
        "    (\"What is my name?\", [\"Alex\", \"Chen\"]),\n",
        "    (\"What level am I?\", [\"L5\", \"senior\"]),\n",
        "    (\"How long have I been at the company?\", [\"6\"]),\n",
        "    (\"What conference am I planning to attend?\", [\"KubeCon\"]),\n",
        "    (\"What's my gym reimbursement?\", [\"75\"]),\n",
        "]\n",
        "\n",
        "print(\"üß™ Memory Accuracy Test\")\n",
        "print(\"(Testing if om-memory recalls facts from early turns after compression)\\n\")\n",
        "\n",
        "passed = 0\n",
        "for question, keywords in recall_tests:\n",
        "    memory_ctx = await om.aget_context(thread_id)\n",
        "    results = collection.query(query_texts=[question], n_results=2)\n",
        "    kb_context = \"\\n\".join(results['documents'][0])\n",
        "    \n",
        "    resp = await oai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": f\"You are an HR assistant.\\nKB: {kb_context}\\n{memory_ctx}\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ],\n",
        "        max_tokens=80,\n",
        "    )\n",
        "    answer = resp.choices[0].message.content\n",
        "    found = any(kw.lower() in answer.lower() for kw in keywords)\n",
        "    if found: passed += 1\n",
        "    print(f\"  {'‚úÖ' if found else '‚ùå'} {question}\")\n",
        "    print(f\"     ‚Üí {answer[:80]}\")\n",
        "\n",
        "accuracy = (passed / len(recall_tests)) * 100\n",
        "print(f\"\\nüìä Memory Accuracy: {passed}/{len(recall_tests)} ({accuracy:.0f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: See What om-memory Stored\n",
        "\n",
        "Let's peek inside om-memory to see the compressed observations it created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "observations = await om.aget_observations(thread_id)\n",
        "messages = await om.storage.aget_messages(thread_id)\n",
        "\n",
        "print(f\"üì¶ om-memory state:\")\n",
        "print(f\"  Observations: {len(observations)} (compressed from 20 turns)\")\n",
        "print(f\"  Messages retained: {len(messages)} (rolling window)\")\n",
        "print(f\"  Background tokens used: {bg_tokens:,}\\n\")\n",
        "\n",
        "print(\"üîç Stored Observations:\")\n",
        "for obs in observations:\n",
        "    print(f\"  {obs.priority.value} {obs.content}\")\n",
        "    \n",
        "await om.aclose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How It Works: Traditional RAG vs om-memory\n",
        "\n",
        "| | Traditional RAG | om-memory |\n",
        "|---|---|---|\n",
        "| **Context growth** | Linear ‚Äî O(n) per turn | Flat ‚Äî O(1) per turn |\n",
        "| **Memory mechanism** | Full history in every call | Compressed observations |\n",
        "| **Token cost** | Grows with conversation | Stays stable |\n",
        "| **Accuracy** | 100% (has everything) | High (95%+ in testing) |\n",
        "| **Long conversations** | Hits context limit | Unlimited |\n",
        "\n",
        "### Architecture\n",
        "```\n",
        "Traditional RAG:    [System Prompt] + [KB Chunks] + [ALL Messages]\n",
        "                                                    ^^^^^^^^^^^^^^^^\n",
        "                                                    grows every turn!\n",
        "\n",
        "om-memory:          [System Prompt] + [KB Chunks] + [Observations] + [Last 2 Messages]\n",
        "                                                    ^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^\n",
        "                                                    stable/cached    tiny rolling window\n",
        "```\n",
        "\n",
        "### Install om-memory\n",
        "```bash\n",
        "pip install om-memory\n",
        "```\n",
        "\n",
        "üì¶ [PyPI](https://pypi.org/project/om-memory/) ¬∑ üêô [GitHub](https://github.com/om-memory/om-memory)"
      ]
    }
  ]
}
